

### Mainlining Databases: Supporting Fast Transactional Workloads on Universal Columnar Data File Formats

* [noisepage](https://db.cs.cmu.edu/papers/2020/p534-li.pdf) [arxiv](https://arxiv.org/abs/2004.14471)

4 Block Transformation

As discussed in Sec. 2.2, the primary obstacle to running transactions on Arrow is write amplification. Our system uses a relaxed Arrow format to achieve good write performance and then uses a lightweight transformation step to put a block into the full Arrow format once it is cold. We now describe this modified format, present our algorithm for transforming them, and discuss the important assumptions and implementation details in NoisePage.

4.1 Relaxed Columnar Format

Typical OLTP workloads modify only a small portion of a database at any given time, while the other parts of the database are mostly accessed by read-only queries [41]. Therefore, for the hot portion, we can trade off read speed for write performance at only a small impact on the overall read performance of the DBMS. To achieve this, we modify the Arrow format for update performance in the hot portion. We detail these changes in this subsection.

Arrow has two sources of write amplification: (1) it disallows gaps in a column, and (2) it stores variable-length values consecutively in a single buffer. Our relaxed format adds a validity bitmap in the block header and additional metadata for each variable-length value to overcome them. As shown in Fig. 5, within a VarlenEntry field, the system maintains 4 bytes for size and 8 bytes for a pointer to the underlying value. Each VarlenEntry is padded to 16 bytes for alignment reasons, and the additional 4 bytes stores a prefix of the value. If a value is shorter than 12 bytes, the system stores it entirely within the object, writing into the pointer. Transactions only access the VarlenEntry instead of Arrow storage directly. Relaxing adherence to Arrow’s format allows the system to only write updates to VarlenEntry, turning a variable-length update into a constant-time fixed-length one, as shown in Fig. 6.

Any readers accessing Arrow storage will be oblivious to the update in VarlenEntry. The system adds a status flag and counter in block headers to coordinate access. A block in NoisePage can be in one of three states – hot, cooling, or frozen. Hot blocks are actively worked on by transactions, whereas frozen blocks are available for in-place scans in the Arrow format; cooling blocks are in the process of being transformed. The access counter on each block functions as a shared latch – each in-place reader adds one to the counter when starting a scan and subtract one when finished. When a transaction updates a frozen block, it first sets that block’s status flag to hot, forcing any future readers to materialize instead of reading in-place. It then spins on the counter and waits for lingering readers to leave the block before proceeding with the update. Once the block is hot, transactional access elide latch protection and rely on the MVCC implementation for thread safety. Other than flipping the flag, there is no transformation process required for a transaction to modify a frozen block because our relaxed format is a generalization of the original Arrow format. Once a block is hot, it remains so until a background process transforms it back to full Arrow compliance.

We now provide an overview of our transformation algorithm, also illustrated in Fig. 7. There are two components of our transformation pipeline, the access observer and block transformer, shown as boxes with dashed lines in Fig. 7. The access observer piggy-backs on the DBMS’s normal garbage collection to inspect recent changes and identify any candidates for transformation to push onto a queue (Sec. 4.2). The block transformer polls from the queue. For correctness reasons, the transformer processes each block at least twice before emitting them as Arrow blocks. The first pass is transactional and rearranges tuples within blocks so that they are contiguous. This transaction passes through the access observer and prompts the access observer to enqueue the block again (Sec. 4.3).

4.2 Identifying Cold Blocks

The DBMS maintains statistics about each block to determine if it is cooling. Collecting them as transactions operate on the database adds overhead to the critical path [33, 36], which is unacceptable for OLTP workloads. Our system trades the quality of such statistics for better performance and then accounts for potential mistakes from this in our transformation algorithm.

A simple heuristic is to mark blocks that have not been modified for some threshold time as cold for each table. Instead of measuring this on the transaction’s critical path, our system takes advantage of the GC’s scan through undo records (Sec. 3.3). From each undo record, the system obtains the modification type (i.e., delete, insert, update) and the corresponding TupleSlot. Time measurement, however, is difficult because the system cannot measure how much time has elapsed between the modification and invocation of the GC. The DBMS instead approximates this by using a coarse-grained counter that periodically increments in GC (e.g. every 10 ms). If transactions have a lifetime shorter than the frequency of this "GC clock", the approximated time
is never earlier than the actual modification and is late by at most one tick, which is good enough for short-lived OLTP transactions [51]. Once the system identifies a cold block, it adds the block to a queue for background processing.

Under this scheme, one thread may identify a block as cold by mistake when another thread is updating it due to delays in access observation. The DBMS reduces the impact of this by ensuring that the transformation algorithm is fast and lightweight. There are two failure cases: (1) a user transaction aborts due to conflicts with the transformation process or (2) the user transaction stalls. There is no way to safely eliminate both cases. Our solution is a two-phase algorithm. The first phase is transactional and operates on a microsecond scale, minimizing the possibility of aborts. The second phase eventually takes a block-level lock for a short critical section but yields to user transactions whenever possible.

4.3 Transformation Algorithm

Once the system identifies cooling blocks, it performs two transfor- mation passes to prepare the block for Arrow readers. The DBMS first needs to compact each block to eliminate any gaps, and then copy variable-length values into a new contiguous buffer for the Arrow varlen representation. There are three approaches to ensure safety with concurrent transactions: (1) block copying, (2) transac- tional operations, or (3) block-level locks. None of these is ideal. The first is expensive, especially when most of the data is not changed. The second adds additional overhead and increases aborts. The third stalls user transactions and limits concurrency in the typical case even without transformation. As shown in Fig. 7, NoisePage uses a hybrid two-phase approach that combines transactional tuple movement and raw operations under exclusive access, which is or- chestrated with a novel multi-stage locking scheme that cooperates with GC to guard against races. We extend the block status flag with two additional values: cooling and freezing. The former indicates that the transformation thread intends to lock, while the latter serves as an exclusive lock that blocks user transactions. Alg. 1 contains the pseudo-code for this operation.

The block transformer continually polls from the transform queue for new blocks to process. When a hot block arrives, the block transformer assigs it to a compaction group, a collection of blocks with the same layout. Within a group, the system uses tu- ples from less-than-full blocks to fill gaps in others and recycle blocks when they become empty. Larger compaction groups are more memory-efficient but also slower to compact. The DBMS uses one transaction per group in this phase to perform all operations; moving is equivalent to deleting and then inserting. If the transac- tion executes without conflicts, it marks the block as cooling and commits the transaction. User transactions compare-and-swap the flag back to hot when modifying a cooling block.

The system now takes a cooling block and formats it into Arrow. The core challenge here is that user transactions can trigger a check- and-miss race — the flip to cooling can be interleaved between a user’s check for the flag and subsequent modification. NoisePage guards against this with the GC, which does not prune any versions visible to running transactions. Because a user transaction must be concurrent with compaction to be susceptible to the race, the compaction transaction’s versions must remain. Therefore, the block transformer safely flips the block status to freezing only if it sees no versioned entry at the end of the scan. Any modifications concurrent to the scan changes the status to hot, which will be detected by the final compare-and-swap to freezing.

After the transformation algorithm obtains exclusive access to the block, it scans each variable-length column to concatenate values into a contiguous buffer and update pointers without trans- actional protection. In the same pass, it also computes metadata information, such as null count into the Arrow block header. When the process is complete, the system marks the block as frozen and allow in-place readers. Although transactional writes are not al- lowed, reads still proceed as the formatting phase changes only the physical location of values and not the logical content of the table. Because a write to any aligned 8-byte address is atomic [2], reads are never unsafe as the DBMS aligns all attributes within a block.

Lastly, we describe how to shuffle tuples within a compaction group (Compact from Alg. 1); pseudo-code is given in Alg. 2. At the end of this routine, tuples in the group are logically contiguous; a group consisting of t tuples with b blocks each with s slots now have 􏰀 st 􏰁 many blocks completely filled, one block filled from beginning to the (t mods)-th slot, and all others empty. The DBMS first sorts the blocks by the number of empty slots and then fills the empty slots from earlier blocks with tuples from later blocks one-by-one.

We measure the efficiency of our algorithm by the number of movements it performs; each movement can trigger index updates, which have performance implications. We show that the above algorithm is at most (t mods) movements worse than optimal. It selects a block set F to be the 􏰀 st 􏰁 blocks that are filled in the final state and a block p that is partially filled and hold t mod s tuples. The rest of the blocks, E, are left empty. It then fills all gaps within F ∪ {p} using tuples from E ∪ {p}, and reorder tuples within p to

make them contiguous.Let Gapf be the set of unfilled slots in a
block f , Gapf′ be the set of unfilled slots in the first t mod s slots in
a block f , Filledf be the set of filled slots in f , and Filledf′ be the
set of filled slots not in the first t mod s slots in f . 

Then, for any valid selection of F , p, and E, [omit math equation from this description]
because there are only t tuples in total. Therefore, an optimal movement is any one-to-one movement between...g

The algorithm constructs F by picking the blocks with the most filled slots. Every gap in F needs to be filled with one movement, and our selection of F results in fewer movements than any other choice. In the worst case, our chosen p, which is the block with the next most filled slots, is empty in the first (t mods) slots. The optimal one is filled, resulting in at most (t mods) movements from the optimal for our algorithm. To achieve the optimal solution, the algorithm needs to scan through the blocks to find an optimal p by trying every possible candidate. From our experiments in Sec. 6, we observe only a marginal reduction in movements, which does not justify the overhead.

### 4.4 Additional Considerations

Now that we have presented our algorithm for transforming cold blocks into Arrow, we demonstrate its flexibility by discussing alternative formats for our transformation algorithm. We also give a more detailed description of the issues in memory management and scaling for larger workloads.

##### Alternative Formats

It is possible to change the formatting phase to emit a different format, although the algorithm performs best if the target format is close to our transactional representation. For example, the system can emit Parquet files by encoding a block and writing to disk at the end of the formatting phase, while re- taining the memory content for efficient read access. To illustrate this capability, we implement an alternative columnar format with dictionary compression [38] similar to Parquet [10] and ORC [9]. The system instead creates a dictionary and an array of dictionary codes. The only difference is that within the critical section of the formatting phase, the algorithm now scans through the block twice – the first to build a dictionary corpus and replace pointers within VarlenEntrys to point to dictionary words and the second to sort the dictionary and build an array of dictionary codes.

##### Workload Assumptions

Our algorithm assumes that blocks are modified frequently for a short time before cooling down and becoming read-only. For write-mostly workloads, blocks are never considered cooling by the access observer, and therefore our scheme adds no benefit or overhead. It is conceivable, however, for some workloads to break this assumption by periodically switching be- tween read-only and write-heavy workloads on a set of blocks. In this case, performance impact on transactions is mitigated as our transformation algorithm completes within milliseonds and allows writers to quickly update frozen blocks in place by resetting the flag; this is in contrast to earlier work [41] that are more heavy-weight and only allows for read-copy-update to frozen blocks.

##### Memory Management

Because the algorithm never blocks readers, the system cannot deallocate memory after the transforma- tion process as its contents are visible to concurrent transactions. In the compaction phase, because writes are transactional, the GC can handle memory management. When moving tuples, the system makes a deep copy of the variable-length values to avoid reasoning about buffer ownership transfer. In the gathering phase, we extend our GC to accept arbitrary actions associated with a timestamp in the form of a callback, which it promises to invoke after the oldest alive transaction in the system is started after the given timestamp. As discussed in Sec. 3.3, this is similar to epoch protection [32]. The system registers an action that reclaims memory for this gathering phase with a timestamp that the compaction thread takes after it completes all of its in-place modifications. This delayed reclamation ensures no transaction reads freed memory.

##### Scaling Transformation and GC

A single GC or transforma- tion thread cannot keep up with transaction throughput from many worker threads, and the DBMS can leverage partitioning opportu- nities for parallelization. For GC, the DBMS uses a transaction’s identifier to assign it to a GC thread. Although the version chain pruning is thread-safe, multiple GC threads pruning the same chain can do so with an out-of-sync view of the current safe timestamp, and incorrectly deallocate different parts of the chain. Each GC thread starts an empty transaction in between refreshes of the global safe timestamp, which prevents other threads from deallo- cating parts of the chain it traverses. To parallelize transformation, the DBMS spawns multiple threads to poll from the same under- lying queue. Because the transformation is independent across compaction groups, the threads never interfere with each other.